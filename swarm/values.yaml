# Default values for swarm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

swarm:
  # Metrics: If enabled it will deploy a InfluxDB + Grafana stack.
  # All swarm nodes will forward metrics to the deploy InfluxDB instance
  metricsEnabled: true
  # Tracing: If enabled, jaeger instance will be deployed.
  tracingEnabled: false
  # Profiling: Enable PPROF
  profilingEnabled: false
  # Image configuration
  image:
    repository: ethersphere/swarm
    tag: latest
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  imagePullPolicy: Always
  ## (dict) If specified, apply these annotations to the swarm pods
  # podAnnotations:
  #   fluentbit.io/exclude: "true"
  # How many swarm nodes we want to run
  replicaCount: 2
  # Pod management policy
  # https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady  # Choose between "OrderedReady" or "Parallel"
  # Update strategy:
  # https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  updateStrategy:
    type: "RollingUpdate"
   # Duration in seconds the pod needs to terminate gracefully
  terminationGracePeriodSeconds: 30
  config:
    # ENS API endpoint
    ens_api: http://geth-host:8545
    # Logging verbosity: 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=detail
    verbosity: 3
    # Prepends log messages with call-site location (file and line number)
    debug: false
    # Maximum number of network peers (network disabled if set to 0)
    maxpeers: 25
    # Network identifier (integer, default 3=swarm testnet)
    bzznetworkid: 3
    # LevelDB Store size - by default 4kb * 10^5 == 400 megabytes
    storesize: 100000
    # List of bootnodes. Example configuration:
    #bootnodes:
    # - enode://xxxxxx@10.0.1.5:30399
    # - enode://yyyyyy@10.0.1.7:30399
    bootnodes: []
    extraFlags: []
    # Pod specific flags (Key: Pod name, Value: flags)
    # With this you can provide certain pods with flags without applying it to all the others.
    # Important: These flags will be visible under the /flags/ directory on all your pods.
    #            Be careful if you use this to provide secret information. All pods can see the flags.
    podSpecificFlags: {}
      # swarm-0: --store.cache.size=10000 --maxpeers=100
      # swarm-5: --maxpeers=60

  secrets:
     # Used to setup a sample Ethereum account in the data directory.
     # If a data directory is mounted with a volume, the first Ethereum account from it
     # is loaded, and Swarm will try to decrypt it non-interactively with this password.
    password: qwerty

  persistence:
    ## this enables PVC templates that will create one per pod
    enabled: false
    ## swarm data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    accessMode: ReadWriteOnce
    size: 8Gi

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources:
    requests:
      memory: 256Mi
      cpu: 0.1
    limits:
      memory: 1024Mi
      cpu: 0.3
  ## Node labels and tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature
  nodeSelector: {}
  tolerations: []
  affinity: {}

  ingress:
    enabled: true
    tls:
      enabled: false
      acmeEnabled: false # Use cert-manager to generate certificates via ACME
    domain: default.swarm-gateways.net
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 20m

  # Configuration for the tracing agent
  tracingAgent:
    # The agent will run within the Swarm pod
    image:
      repository: jaegertracing/jaeger-agent
      tag: 1.11.0
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        memory: 64Mi
        cpu: 0.1
      limits:
        memory: 64Mi
        cpu: 0.1

jaeger:
  image:
    repository: jaegertracing/all-in-one
    tag: 1.11.0
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  imagePullPolicy: IfNotPresent
  replicaCount: 1
  service:
    query:
      type: ClusterIP
    agent:
      type: ClusterIP
  config:
    # Custom ENV and CLI args passed to Jaeger
    env:
      SPAN_STORAGE_TYPE: memory
    args:
      - "--memory.max-traces=100000"
  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources:
    requests:
      memory: 4Gi
      cpu: 0.5
    limits:
      memory: 5Gi
      cpu: 1
  ## Node labels and tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature
  nodeSelector: {}
  tolerations: []
  affinity: {}


##
## Influxdb Dependency
##

influxdb:
  ## influxdb image version
  ## ref: https://hub.docker.com/r/library/influxdb/tags/
  image:
    repo: "influxdb"
    tag: "1.7.6-alpine"
    pullPolicy: IfNotPresent
    ## If specified, use these secrets to access the images
    # pullSecrets:
    #   - registry-secret

  ## Specify a service type
  ## ref: http://kubernetes.io/docs/user-guide/services/
  ##
  service:
    ## Add annotations to service
    # annotations: {}
    type: ClusterIP
    ## Add IP Cluster
    # clusterIP: ""
    ## Add external IPs that route to one or more cluster nodes
    # externalIPs: []
    ## Specify LoadBalancer IP (only allow on some cloud provider)
    # loadBalancerIP: ""
    ## Allow source IPs to access on service (if empty, any access allow)
    # loadBalancerSourceRanges: []

  ## Persist data to a persistent volume
  ##
  persistence:
    enabled: false
    ## If true will use an existing PVC instead of creating one
    # useExisting: false
    ## Name of existing PVC to be used in the influx deployment
    # name:
    ## influxdb data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    accessMode: ReadWriteOnce
    size: 8Gi

  ## Create default user through Kubernetes job
  ## Defaults indicated below
  ##
  setDefaultUser:
    enabled: true

    ## Image of the container used for job
    ## Default: appropriate/curl:latest
    ##
    image: appropriate/curl:latest

    ## Deadline for job so it does not retry forever.
    ## Default: activeDeadline: 300
    ##
    activeDeadline: 300

    ## Restart policy for job
    ## Default: OnFailure
    restartPolicy: OnFailure

    user:

      ## The user name
      ## Default: "admin"
      username: "swarm"

      ## User password
      ## single quotes must be escaped (\')
      ## Default:
      password: "swarm"

      ## User privileges
      ## Default: "WITH ALL PRIVILEGES"
      privileges: "WITH ALL PRIVILEGES"

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources:
    requests:
      memory: 256Mi
      cpu: null
    limits:
      memory: 16Gi
      cpu: null

  ingress:
    enabled: false
    tls: false
    # secretName: my-tls-cert # only needed if tls above is true
    hostname: influxdb.foobar.com
    annotations:
      # kubernetes.io/ingress.class: "nginx"
      # kubernetes.io/tls-acme: "true"

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  # - key: "key"
  #   operator: "Equal|Exists"
  #   value: "value"
  #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## The InfluxDB image uses several environment variables to automatically
  ## configure certain parts of the server.
  ## Ref: https://hub.docker.com/_/influxdb/
  env:
  - name: INFLUXDB_LOGGING_LEVEL
    value: "warn"
  ## Change InfluxDB configuration parameters below:
  ## Defaults are indicated
  ## ref: https://docs.influxdata.com/influxdb/v1.1/administration/config/
  config:
    reporting_disabled: false

    storage_directory: /var/lib/influxdb
    rpc:
      enabled: true
      bind_address: 8088
    meta:
      retention_autocreate: true
      logging_enabled: true
    data:
      query_log_enabled: false
      cache_max_memory_size: 1073741824
      cache_snapshot_memory_size: 26214400
      cache_snapshot_write_cold_duration: 10m0s
      compact_full_write_cold_duration: 4h0m0s
      max_series_per_database: 1000000
      max_values_per_tag: 100000
      trace_logging_enabled: false
    coordinator:
      write_timeout: 10s
      max_concurrent_queries: 0
      query_timeout: 0s
      log_queries_after: 0s
      max_select_point: 0
      max_select_series: 0
      max_select_buckets: 0
    retention:
      enabled: true
      check_interval: 30m0s
    shard_precreation:
      enabled: true
      check_interval: 10m0s
      advance_period: 30m0s
    admin:
      enabled: false
      bind_address: 8083
      https_enabled: false
      https_certificate: /etc/ssl/influxdb.pem
    monitor:
      store_enabled: true
      store_database: _internal
      store_interval: 10s
    subscriber:
      enabled: true
      http_timeout: 30s
      insecure_skip_verify: false
      ca_certs: ""
      write_concurrency: 40
      write_buffer_size: 1000
    http:
      enabled: true
      bind_address: 8086
      auth_enabled: false
      flux_enabled: true
      log_enabled: false
      write_tracing: false
      pprof_enabled: true
      https_enabled: false
      https_certificate: /etc/ssl/influxdb.pem
      https_private_key: ""
      max_row_limit: 10000
      max_connection_limit: 0
      shared_secret: "beetlejuicebeetlejuicebeetlejuice"
      realm: InfluxDB
      unix_socket_enabled: false
      bind_socket: /var/run/influxdb.sock
    graphite:
      enabled: false
      bind_address: 2003
      database: graphite
      retention_policy: autogen
      protocol: tcp
      batch_size: 5000
      batch_pending: 10
      batch_timeout: 1s
      consistency_level: one
      separator: .
      udp_read_buffer: 0
      # Uncomment to define graphite templates
      # templates:
      #   - "graphite.metric.*.*.* measurement.run"
    collectd:
      enabled: false
      bind_address: 25826
      database: collectd
      retention_policy: autogen
      batch_size: 5000
      batch_pending: 10
      batch_timeout: 10s
      read_buffer: 0
      typesdb: /usr/share/collectd/types.db
      security_level: none
      auth_file: /etc/collectd/auth_file
    opentsdb:
      enabled: false
      bind_address: 4242
      database: opentsdb
      retention_policy: autogen
      consistency_level: one
      tls_enabled: false
      certificate: /etc/ssl/influxdb.pem
      batch_size: 1000
      batch_pending: 5
      batch_timeout: 1s
      log_point_errors: true
    udp:
      enabled: false
      bind_address: 8089
      database: udp
      retention_policy: autogen
      batch_size: 5000
      batch_pending: 10
      read_buffer: 0
      batch_timeout: 1s
      precision: "ns"
    continuous_queries:
      log_enabled: true
      enabled: true
      run_interval: 1s
    logging:
      format: auto
      level: info
      supress_logo: false

  # Allow executing custom init scripts
  #
  # If the container finds any files with the extensions .sh or .iql inside of the
  # /docker-entrypoint-initdb.d folder, it will execute them. The order they are
  # executed in is determined by the shell. This is usually alphabetical order.
  initScripts:
    enabled: true
    scripts:
      init.iql: |+
        CREATE DATABASE "metrics" WITH DURATION 30d REPLICATION 1 NAME "autogen"

  backup:
    enabled: false
    schedule: "0 0 * * *"
    annotations: {}
    destination: gs://bucket/influxdb
    # Google Cloud Storage
    gcs:
      serviceAccountSecret: influxdb-backup-key


##
## Grafana dependency
##
grafana:
  rbac:
    create: false
    pspEnabled: false
    pspUseAppArmor: true
    namespaced: false
  serviceAccount:
    create: true
    name:
    nameTest:

  replicas: 1

  ## See `kubectl explain deployment.spec.strategy` for more
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  deploymentStrategy:
    type: RollingUpdate

  readinessProbe:
    httpGet:
      path: /api/health
      port: 3000

  livenessProbe:
    httpGet:
      path: /api/health
      port: 3000
    initialDelaySeconds: 60
    timeoutSeconds: 30
    failureThreshold: 10

  image:
    repository: grafana/grafana
    tag: 6.2.5
    pullPolicy: IfNotPresent

    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - myRegistrKeySecretName

  testFramework:
    image: "dduportal/bats"
    tag: "0.4.0"
    securityContext: {}

  securityContext:
    runAsUser: 472
    fsGroup: 472


  extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /etc/grafana/ssl/
    #   subPath: certificates.crt # (optional)
    #   configMap: certs-configmap
    #   readOnly: true


  extraEmptyDirMounts: []
    # - name: provisioning-notifiers
    #   mountPath: /etc/grafana/provisioning/notifiers

  ## Assign a PriorityClassName to pods if set
  # priorityClassName:

  downloadDashboardsImage:
    repository: appropriate/curl
    tag: latest
    pullPolicy: IfNotPresent

  downloadDashboards:
    env: {}

  ## Pod Annotations
  # podAnnotations: {}

  ## Deployment annotations
  # annotations: {}

  ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service).
  ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.
  ## ref: http://kubernetes.io/docs/user-guide/services/
  ##
  service:
    type: ClusterIP
    port: 80
    targetPort: 3000
      # targetPort: 4181 To be used with a proxy extraContainer
    annotations: {}
    labels: {}

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    labels: {}
    path: /
    hosts:
      - chart-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  resources: {}
  #  limits:
  #    cpu: 100m
  #    memory: 128Mi
  #  requests:
  #    cpu: 100m
  #    memory: 128Mi

  ## Node labels for pod assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  #
  nodeSelector: {}

  ## Tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []

  ## Affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  extraInitContainers: []

  ## Enable an Specify container in extraContainers. This is meant to allow adding an authentication proxy to a grafana pod
  extraContainers: |
  # - name: proxy
  #   image: quay.io/gambol99/keycloak-proxy:latest
  #   args:
  #   - -provider=github
  #   - -client-id=
  #   - -client-secret=
  #   - -github-org=<ORG_NAME>
  #   - -email-domain=*
  #   - -cookie-secret=
  #   - -http-address=http://0.0.0.0:4181
  #   - -upstream-url=http://127.0.0.1:3000
  #   ports:
  #     - name: proxy-web
  #       containerPort: 4181

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    enabled: false
    # storageClassName: default
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    # annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection
    # subPath: ""
    # existingClaim:

  initChownData:
    ## If false, data ownership will not be reset at startup
    ## This allows the prometheus-server to be run with an arbitrary user
    ##
    enabled: true

    ## initChownData container image
    ##
    image:
      repository: busybox
      tag: "1.30"
      pullPolicy: IfNotPresent

    ## initChownData resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
    #  limits:
    #    cpu: 100m
    #    memory: 128Mi
    #  requests:
    #    cpu: 100m
    #    memory: 128Mi


  # Administrator credentials when not using an existing secret (see below)
  adminUser: swarm
  adminPassword: swarm

  # Use an existing secret for the admin user.
  admin:
    existingSecret: ""
    userKey: admin-user
    passwordKey: admin-password

  ## Define command to be executed at startup by grafana container
  ## Needed if using `vault-env` to manage secrets (ref: https://banzaicloud.com/blog/inject-secrets-into-pods-vault/)
  ## Default is "run.sh" as defined in grafana's Dockerfile
  # command:
  # - "sh"
  # - "/run.sh"

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  ## Extra environment variables that will be pass onto deployment pods
  env: {}

  ## The name of a secret in the same kubernetes namespace which contain values to be added to the environment
  ## This can be useful for auth tokens, etc
  envFromSecret: ""

  ## Additional grafana server secret mounts
  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
  extraSecretMounts: []
    # - name: secret-files
    #   mountPath: /etc/secrets
    #   secretName: grafana-secret-files
    #   readOnly: true

  ## Additional grafana server volume mounts
  # Defines additional volume mounts.
  extraVolumeMounts: []
    # - name: extra-volume
    #   mountPath: /mnt/volume
    #   readOnly: true
    #   existingClaim: volume-claim


  ## Pass the plugins you want installed as a list.
  ##
  plugins: []
    # - digrich-bubblechart-panel
    # - grafana-clock-panel

  ## Configure grafana datasources
  ## ref: http://docs.grafana.org/administration/provisioning/#datasources
  ##
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: metrics
        type: influxdb
        url: http://swarm-influxdb:8086
        database: metrics
        user: swarm
        password: swarm
        access: proxy
        isDefault: true

  ## Configure notifiers
  ## ref: http://docs.grafana.org/administration/provisioning/#alert-notification-channels
  ##
  notifiers: {}
  #  notifiers.yaml:
  #    notifiers:
  #    - name: email-notifier
  #      type: email
  #      uid: email1
  #      # either:
  #      org_id: 1
  #      # or
  #      org_name: Main Org.
  #      is_default: true
  #      settings:
  #        addresses: an_email_address@example.com
  #    delete_notifiers:

  ## Configure grafana dashboard providers
  ## ref: http://docs.grafana.org/administration/provisioning/#dashboards
  ##
  ## `path` must be /var/lib/grafana/dashboards/<provider_name>
  ##
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default

  ## Configure grafana dashboard to import
  ## NOTE: To use dashboards you must also enable/configure dashboardProviders
  ## ref: https://grafana.com/dashboards
  ##
  ## dashboards per provider, use provider name as key.
  ##
  dashboards:
    default:
      smoke:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/smoke.json
      ldbstore:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/ldbstore.json
      swarm:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/swarm.json
      upload-speed:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/upload-speed.json
      sliding:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/smoke-sliding.json
      kad:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/kad.json
      fetchers:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/fetchers.json
      timeouts:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/timeouts.json
      p2p:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/p2p.json
      pushsync:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/pushsync.json
      runtime:
        url: https://raw.githubusercontent.com/ethersphere/grafana-dashboards/master/runtime.json

  ## Reference to external ConfigMap per provider. Use provider name as key and ConfiMap name as value.
  ## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both.
  ## ConfigMap data example:
  ##
  ## data:
  ##   example-dashboard.json: |
  ##     RAW_JSON
  ##
  dashboardsConfigMaps: {}
  #  default: ""

  ## Grafana's primary configuration
  ## NOTE: values in map will be converted to ini format
  ## ref: http://docs.grafana.org/installation/configuration/
  ##
  grafana.ini:
    paths:
      data: /var/lib/grafana/data
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: false
    log:
      mode: console
    grafana_net:
      url: https://grafana.net
    auth.anonymous:
      enabled: true

  ## LDAP Authentication can be enabled with the following values on grafana.ini
  ## NOTE: Grafana will fail to start if the value for ldap.toml is invalid
    # auth.ldap:
    #   enabled: true
    #   allow_sign_up: true
    #   config_file: /etc/grafana/ldap.toml

  ## Grafana's LDAP configuration
  ## Templated by the template in _helpers.tpl
  ## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled
  ## ref: http://docs.grafana.org/installation/configuration/#auth-ldap
  ## ref: http://docs.grafana.org/installation/ldap/#configuration
  ldap:
    # `existingSecret` is a reference to an existing secret containing the ldap configuration
    # for Grafana in a key `ldap-toml`.
    existingSecret: ""
    # `config` is the content of `ldap.toml` that will be stored in the created secret
    config: ""
    # config: |-
    #   verbose_logging = true

    #   [[servers]]
    #   host = "my-ldap-server"
    #   port = 636
    #   use_ssl = true
    #   start_tls = false
    #   ssl_skip_verify = false
    #   bind_dn = "uid=%s,ou=users,dc=myorg,dc=com"

  ## Grafana's SMTP configuration
  ## NOTE: To enable, grafana.ini must be configured with smtp.enabled
  ## ref: http://docs.grafana.org/installation/configuration/#smtp
  smtp:
    # `existingSecret` is a reference to an existing secret containing the smtp configuration
    # for Grafana.
    existingSecret: ""
    userKey: "user"
    passwordKey: "password"

  ## Sidecars that collect the configmaps with specified label and stores the included files them into the respective folders
  ## Requires at least Grafana 5 to work and can't be used together with parameters dashboardProviders, datasources and dashboards
  sidecar:
    image: kiwigrid/k8s-sidecar:0.1.20
    imagePullPolicy: IfNotPresent
    resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 100Mi
  #   requests:
  #     cpu: 50m
  #     memory: 50Mi
    # skipTlsVerify Set to true to skip tls verification for kube api calls
    # skipTlsVerify: true
    dashboards:
      enabled: false
      # label that the configmaps with dashboards are marked with
      label: grafana_dashboard
      # folder in the pod that should hold the collected dashboards (unless `defaultFolderName` is set)
      folder: /tmp/dashboards
      # The default folder name, it will create a subfolder under the `folder` and put dashboards in there instead
      defaultFolderName: null
      # If specified, the sidecar will search for dashboard config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null
      # provider configuration that lets grafana manage the dashboards
      provider:
        # name of the provider, should be unique
        name: sidecarProvider
        # orgid as configured in grafana
        orgid: 1
        # folder in which the dashboards should be imported in grafana
        folder: ''
        # type of the provider
        type: file
        # disableDelete to activate a import-only behaviour
        disableDelete: false
    datasources:
      enabled: false
      # label that the configmaps with datasources are marked with
      label: grafana_datasource
      # If specified, the sidecar will search for datasource config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null
